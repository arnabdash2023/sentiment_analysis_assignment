{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8f1e19a4-6023-40ff-937c-b89eb43337a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing all dependincies\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import spacy\n",
    "# make sure to have spacy english nlp model downloaded if not run \"python -m spacy download en_core_web_sm\" to download\n",
    "import codecs\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "## setting input variable to the location of input File provided in EXCEL format\n",
    "input_filepath = \"./files/Input.xlsx\"\n",
    "## reading excel files using pandas in to a dataframe\n",
    "df = pd.read_excel(input_filepath)\n",
    "\n",
    "##  make a function that scrapes given url and saves that perticular url's content in a text file on the system.\n",
    "def scrap_url(url):\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()  # Raise an exception for non-200 status codes\n",
    "\n",
    "        html = res.text\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        title = soup.find(\"h1\", class_=\"entry-title\")\n",
    "        if title:\n",
    "            title = title.text.strip()\n",
    "\n",
    "        content_div = soup.find(\"div\", class_=\"td-post-content\") or soup.find(\"div\", class_=\"td-main-content\")\n",
    "        if not content_div:\n",
    "            page = soup.find(\"div\", class_=\"wpb_wrapper\")\n",
    "            if page:\n",
    "                title = page.find(\"h1\", class_=\"tdb-title-text\")\n",
    "                if title:\n",
    "                    title = title.text.strip()\n",
    "                content_div = page.find(\"div\", class_=\"tdb-block-inner\")\n",
    "\n",
    "        paragraphs = \"\"\n",
    "        if content_div:\n",
    "            for pre in content_div.find_all(\"pre\"):  # Remove all <pre> tags\n",
    "                pre.decompose()\n",
    "            paragraphs = content_div.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        if title and paragraphs:\n",
    "            url_id = df[\"URL_ID\"][row]\n",
    "            print(url_id)\n",
    "            with open(url_id, \"w\") as tosave:\n",
    "                tosave.write(title + \" \" + paragraphs)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {url} ({e})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "for row in range(len(df)):\n",
    "    url = df['URL'][row]\n",
    "    scrap_url(row)\n",
    "\n",
    "## a function to get all stop words provided along the assignment in python list\n",
    "def get_stop_words():\n",
    "    stop_words = []\n",
    "    filenames = [\"StopWords_Auditor.txt\", \"StopWords_Currencies.txt\", \"StopWords_DatesandNumbers.txt\", \"StopWords_GenericLong.txt\", \"StopWords_Generic.txt\", \"StopWords_Geographic.txt\", \"StopWords_Names.txt\"]\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            with open(f\"./files/StopWords/{filename}\", \"r\") as file:\n",
    "                contents = file.read()\n",
    "                if \"|\" in contents:\n",
    "                    stop_words.append(contents.lower().replace(\"|\",\"\\n\") .split(sep=\"\\n\"))\n",
    "                else:\n",
    "                    stop_words.append(contents.lower().split(sep=\"\\n\"))\n",
    "        except UnicodeDecodeError:\n",
    "            with open(f\"./files/StopWords/{filename}\", \"r\", encoding=\"iso-8859-1\") as file:\n",
    "                contents = file.read()\n",
    "                if \"|\" in contents:\n",
    "                    stop_words.append(contents.lower().replace(\"|\",\"\\n\") .split(sep=\"\\n\"))\n",
    "                else:\n",
    "                    stop_words.append(contents.lower().split(sep=\"\\n\"))\n",
    "    flat_list = sum(stop_words, [])\n",
    "    flat_list = [item.strip() for item in flat_list]\n",
    "    return flat_list\n",
    "stop_words = get_stop_words()\n",
    "\n",
    "## a function to get all negative words provided along the assignment in python list\n",
    "def get_negative_words():\n",
    "    negative_words = []\n",
    "    with open(\"./files/MasterDictionary/negative-words.txt\",\"r\", encoding=\"cp1252\") as temp:\n",
    "        t = temp.read()\n",
    "        negative_words = t.splitlines()\n",
    "        negative_words = [item.strip() for item in negative_words]\n",
    "    return negative_words\n",
    "\n",
    "negative_words  = get_negative_words()\n",
    "\n",
    "## a function to get all postive words provided along the assignment in python list\n",
    "\n",
    "def get_positive_words():\n",
    "    positive_words = []\n",
    "    with open(\"./files/MasterDictionary/positive-words.txt\",\"r\") as temp:\n",
    "        t = temp.read()\n",
    "        positive_words = t.splitlines()\n",
    "        positive_words = [item.strip() for item in positive_words]\n",
    "    return positive_words\n",
    "\n",
    "positive_words  = get_positive_words()\n",
    "\n",
    "new_negatives = []\n",
    "new_positives = []\n",
    "\n",
    "for element in negative_words:\n",
    "    if element not in stop_words:\n",
    "        new_negatives.append(element)\n",
    "\n",
    "for element in positive_words:\n",
    "    if element not in stop_words:\n",
    "        new_positives.append(element)\n",
    "\n",
    "## get the paragraphs text from the sraped text file with \"url_id\" as name\n",
    "def get_scrapped_paragraphs(row):\n",
    "    paragraphs = \"\"\n",
    "    url_id = df[\"URL_ID\"][row]\n",
    "    try:\n",
    "        with open(f\"./{url_id}\",\"r\") as file:\n",
    "           paragraphs = file.read()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    return paragraphs\n",
    "def func_positive_score(texts):\n",
    "    return sum(word in texts.lower().split() for word in new_positives)\n",
    "\n",
    "\n",
    "def func_negative_score(texts):\n",
    "    return sum(word in texts.lower().split() for word in new_negatives)\n",
    "\n",
    "\n",
    "def func_polarity_score(texts):\n",
    "    texts = tb(texts)\n",
    "    return texts.sentiment[0]\n",
    "\n",
    "\n",
    "def func_subjectivity_score(texts):\n",
    "    texts = tb(texts)\n",
    "    return texts.sentiment[1]\n",
    "\n",
    "\n",
    "def func_avg_sentence_length(texts):\n",
    "  sentences = texts.sentences\n",
    "  total_words = sum(len(sentence.words) for sentence in sentences)\n",
    "  return total_words / len(sentences) if len(sentences) else 0\n",
    "\n",
    "\n",
    "def func_count_syllables(word):\n",
    "  vowels = \"aeiouy\"\n",
    "  word = word.lower()\n",
    "  syllable_count = 0\n",
    "  for i in range(len(word)):\n",
    "      if word[i] in vowels:\n",
    "          syllable_count += 1\n",
    "          if i != len(word) - 1 and word[i + 1] in vowels and word[i] != \"e\":\n",
    "              syllable_count -= 1\n",
    "  return syllable_count\n",
    "\n",
    "\n",
    "def func_complex_word_percentage(texts):\n",
    "  words = texts.words\n",
    "  total_words = len(words)\n",
    "  complex_words = sum(func_count_syllables(word) >= 3 for word in words)\n",
    "  return (complex_words / (total_words)) * 100\n",
    "\n",
    "\n",
    "def func_fog_index(texts):\n",
    "  avg_sentence_len = func_avg_sentence_length(texts)\n",
    "  percentage_complex_words = func_complex_word_percentage(texts)\n",
    "  return 0.4 * (avg_sentence_len + percentage_complex_words)\n",
    "\n",
    "\n",
    "def func_avg_words_per_sentence(text):\n",
    "  sentences = texts.sentences\n",
    "  return sum(len(sentence.words) for sentence in sentences) / len(sentences) if len(sentences) else 0\n",
    "\n",
    "\n",
    "def func_complex_word_count(texts):\n",
    "  words = texts.words\n",
    "  return sum(func_count_syllables(word) >= 3 for word in words)\n",
    "\n",
    "\n",
    "def func_word_count(texts):\n",
    "  return len(texts.words)\n",
    "\n",
    "\n",
    "def func_syllable_per_word(texts):\n",
    "  words = texts.words\n",
    "  total_syllables = sum(func_count_syllables(word) for word in words)\n",
    "  return total_syllables / len(words) if len(words) else 0\n",
    "\n",
    "\n",
    "def func_avg_word_length(texts):\n",
    "  words = texts.words\n",
    "  return (sum(len(word) for word in words)) / len(words) if len(words) else 0\n",
    "\n",
    "\n",
    "def func_personal_pronouns(texts):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")  # Load the English NLP model\n",
    "    doc = nlp(texts)\n",
    "    personal_pronouns = [\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\", \"myself\", \"yourself\", \"himself\", \"herself\", \"ourselves\", \"yourselves\", \"themselves\"]\n",
    "    pronoun_count = 0\n",
    "    for token in doc:\n",
    "        if token.text.lower() in personal_pronouns and not token.ent_type_:  # Check for personal pronoun and no named entity\n",
    "            pronoun_count += 1\n",
    "    return pronoun_count\n",
    "    output_filepath = \"./output.xlsx\"\n",
    "ouput_df = pd.read_excel(output_filepath)\n",
    "columns = list(ouput_df.columns)\n",
    "for row in range(len(df)):\n",
    "    paragraphs = get_scrapped_paragraphs(row)\n",
    "    if paragraphs:\n",
    "        print(f\"calculating scores for {df['URL_ID'][row]} :\")\n",
    "        paras = \" \".join(word for word in paragraphs.split() if word not in stop_words)\n",
    "        texts = tb(paragraphs)\n",
    "        positive_score = func_positive_score(paras)\n",
    "        negative_score = func_negative_score(paras)\n",
    "        polarity = func_polarity_score(paras)\n",
    "        subjectivity = func_subjectivity_score(paras)\n",
    "        avg_sentence_length = func_avg_sentence_length(texts)\n",
    "        syllable_count = func_count_syllables(texts)\n",
    "        complex_word_percentage = func_complex_word_percentage(texts)\n",
    "        avg_word_length = func_avg_word_length(texts)\n",
    "        personal_pronouns = func_personal_pronouns(paragraphs)\n",
    "        syllable_per_word = func_syllable_per_word(texts)\n",
    "        word_count = func_word_count(texts)\n",
    "        complex_word_count = func_complex_word_count(texts)\n",
    "        avg_words_per_sentence = func_avg_words_per_sentence(texts)\n",
    "        fog_index = func_fog_index(texts)\n",
    "        scores_dict = {'URL_ID': ouput_df['URL_ID'][row] , 'URL': ouput_df['URL'][row], 'POSITIVE SCORE': positive_score, 'NEGATIVE SCORE': negative_score, 'POLARITY SCORE': polarity, 'SUBJECTIVITY SCORE': subjectivity, 'AVG SENTENCE LENGTH': avg_sentence_length, 'PERCENTAGE OF COMPLEX WORDS': complex_word_percentage , 'FOG INDEX':fog_index , 'AVG NUMBER OF WORDS PER SENTENCE':avg_words_per_sentence , 'COMPLEX WORD COUNT':complex_word_count , 'WORD COUNT':word_count , 'SYLLABLE PER WORD':syllable_per_word , 'PERSONAL PRONOUNS':personal_pronouns , 'AVG WORD LENGTH': avg_word_length}\n",
    "        for key,value in scores_dict.items():\n",
    "            ouput_df.at[row, key] = value\n",
    "try:\n",
    "    with pd.ExcelWriter('my_outpul.xlsx') as writer:\n",
    "        ouput_df.to_excel(writer,sheet_name='Sheet1')\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ba24b6e4-a999-478e-8514-f80caab720e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406f2829-0369-4dd5-85df-4441bd494c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3124e-c7e9-4f35-bc7d-d425d7f1087f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8981dc2e-a87c-47e5-bb5e-3d448863c998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f4a37bd-bab5-466b-9220-cdd63274d3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0a5113e5-827b-4db5-a1c5-cb0968a413ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd9fb3-2575-4d01-93c8-119d659b62ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f37589e-11a9-42d1-92f6-fd701caea50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9490e8-5671-43fb-aa02-d43974486bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbb192fc-6e61-4d5e-8c96-5134b2b9054b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "86969866-3bda-4bab-b997-f48cc5eb5ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30471c7-365d-4019-bb8c-333a60d40ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f03382e-1361-4f31-bd4b-8cecedb2eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0e98ed1-4be3-44da-a3ab-49783e63836d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for blackassign0001 values are:\n",
      "for blackassign0002 values are:\n",
      "for blackassign0003 values are:\n",
      "for blackassign0004 values are:\n",
      "for blackassign0005 values are:\n",
      "for blackassign0006 values are:\n",
      "for blackassign0007 values are:\n",
      "for blackassign0008 values are:\n",
      "for blackassign0009 values are:\n",
      "for blackassign0010 values are:\n",
      "for blackassign0011 values are:\n",
      "for blackassign0012 values are:\n",
      "for blackassign0013 values are:\n",
      "for blackassign0015 values are:\n",
      "for blackassign0016 values are:\n",
      "for blackassign0017 values are:\n",
      "for blackassign0018 values are:\n",
      "for blackassign0019 values are:\n",
      "for blackassign0021 values are:\n",
      "for blackassign0022 values are:\n",
      "for blackassign0023 values are:\n",
      "for blackassign0024 values are:\n",
      "for blackassign0025 values are:\n",
      "for blackassign0026 values are:\n",
      "for blackassign0027 values are:\n",
      "for blackassign0028 values are:\n",
      "for blackassign0030 values are:\n",
      "for blackassign0031 values are:\n",
      "for blackassign0032 values are:\n",
      "for blackassign0033 values are:\n",
      "for blackassign0034 values are:\n",
      "for blackassign0035 values are:\n",
      "for blackassign0037 values are:\n",
      "for blackassign0038 values are:\n",
      "for blackassign0039 values are:\n",
      "for blackassign0040 values are:\n",
      "for blackassign0041 values are:\n",
      "for blackassign0042 values are:\n",
      "for blackassign0044 values are:\n",
      "for blackassign0045 values are:\n",
      "for blackassign0046 values are:\n",
      "for blackassign0047 values are:\n",
      "for blackassign0048 values are:\n",
      "for blackassign0050 values are:\n",
      "for blackassign0051 values are:\n",
      "for blackassign0052 values are:\n",
      "for blackassign0053 values are:\n",
      "for blackassign0054 values are:\n",
      "for blackassign0055 values are:\n",
      "for blackassign0056 values are:\n",
      "for blackassign0057 values are:\n",
      "for blackassign0058 values are:\n",
      "for blackassign0059 values are:\n",
      "for blackassign0060 values are:\n",
      "for blackassign0061 values are:\n",
      "for blackassign0062 values are:\n",
      "for blackassign0063 values are:\n",
      "for blackassign0064 values are:\n",
      "for blackassign0065 values are:\n",
      "for blackassign0066 values are:\n",
      "for blackassign0067 values are:\n",
      "for blackassign0068 values are:\n",
      "for blackassign0069 values are:\n",
      "for blackassign0070 values are:\n",
      "for blackassign0071 values are:\n",
      "for blackassign0072 values are:\n",
      "for blackassign0073 values are:\n",
      "for blackassign0074 values are:\n",
      "for blackassign0075 values are:\n",
      "for blackassign0076 values are:\n",
      "for blackassign0077 values are:\n",
      "for blackassign0078 values are:\n",
      "for blackassign0079 values are:\n",
      "for blackassign0080 values are:\n",
      "for blackassign0081 values are:\n",
      "for blackassign0082 values are:\n",
      "for blackassign0085 values are:\n",
      "for blackassign0086 values are:\n",
      "for blackassign0087 values are:\n",
      "for blackassign0088 values are:\n",
      "for blackassign0089 values are:\n",
      "for blackassign0090 values are:\n",
      "for blackassign0091 values are:\n",
      "for blackassign0093 values are:\n",
      "for blackassign0094 values are:\n",
      "for blackassign0095 values are:\n",
      "for blackassign0096 values are:\n",
      "for blackassign0097 values are:\n",
      "for blackassign0098 values are:\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "94a151ed-c0f6-4947-9b08-e815fa834f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "57de2e34-f2c0-412f-8ee1-ada905f04057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.242714</td>\n",
       "      <td>0.603379</td>\n",
       "      <td>15.769231</td>\n",
       "      <td>16.504065</td>\n",
       "      <td>12.909318</td>\n",
       "      <td>15.769231</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>1.710569</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.555285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.077790</td>\n",
       "      <td>0.457205</td>\n",
       "      <td>18.962025</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>19.013382</td>\n",
       "      <td>18.962025</td>\n",
       "      <td>428.0</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.389853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.066737</td>\n",
       "      <td>0.396218</td>\n",
       "      <td>19.196429</td>\n",
       "      <td>38.139535</td>\n",
       "      <td>22.934385</td>\n",
       "      <td>19.196429</td>\n",
       "      <td>410.0</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>2.255814</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.086512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.004925</td>\n",
       "      <td>0.380975</td>\n",
       "      <td>20.803922</td>\n",
       "      <td>35.815269</td>\n",
       "      <td>22.647676</td>\n",
       "      <td>20.803922</td>\n",
       "      <td>380.0</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>2.177191</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.918944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.059091</td>\n",
       "      <td>0.493485</td>\n",
       "      <td>17.666667</td>\n",
       "      <td>28.011611</td>\n",
       "      <td>18.271311</td>\n",
       "      <td>17.666667</td>\n",
       "      <td>193.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>1.956459</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.509434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.034505</td>\n",
       "      <td>0.345489</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>26.283186</td>\n",
       "      <td>19.553274</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>297.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1.894690</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.230973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.038921</td>\n",
       "      <td>0.429680</td>\n",
       "      <td>28.921053</td>\n",
       "      <td>21.201092</td>\n",
       "      <td>20.048858</td>\n",
       "      <td>28.921053</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>1.696087</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.623294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.107535</td>\n",
       "      <td>0.406875</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>28.255528</td>\n",
       "      <td>18.702211</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>115.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>1.889435</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.353808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL  \\\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "..              ...                                                ...   \n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
       "\n",
       "    POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0             13.0             5.0        0.242714            0.603379   \n",
       "1             34.0            19.0        0.077790            0.457205   \n",
       "2             23.0            13.0        0.066737            0.396218   \n",
       "3             23.0            52.0        0.004925            0.380975   \n",
       "4             15.0             7.0        0.059091            0.493485   \n",
       "..             ...             ...             ...                 ...   \n",
       "95            17.0            33.0        0.034505            0.345489   \n",
       "96            14.0            26.0        0.038921            0.429680   \n",
       "97             3.0             2.0        0.107535            0.406875   \n",
       "98             NaN             NaN             NaN                 NaN   \n",
       "99             NaN             NaN             NaN                 NaN   \n",
       "\n",
       "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0             15.769231                    16.504065  12.909318   \n",
       "1             18.962025                    28.571429  19.013382   \n",
       "2             19.196429                    38.139535  22.934385   \n",
       "3             20.803922                    35.815269  22.647676   \n",
       "4             17.666667                    28.011611  18.271311   \n",
       "..                  ...                          ...        ...   \n",
       "95            22.600000                    26.283186  19.553274   \n",
       "96            28.921053                    21.201092  20.048858   \n",
       "97            18.500000                    28.255528  18.702211   \n",
       "98                  NaN                          NaN        NaN   \n",
       "99                  NaN                          NaN        NaN   \n",
       "\n",
       "    AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                          15.769231               203.0      1230.0   \n",
       "1                          18.962025               428.0      1498.0   \n",
       "2                          19.196429               410.0      1075.0   \n",
       "3                          20.803922               380.0      1061.0   \n",
       "4                          17.666667               193.0       689.0   \n",
       "..                               ...                 ...         ...   \n",
       "95                         22.600000               297.0      1130.0   \n",
       "96                         28.921053               233.0      1099.0   \n",
       "97                         18.500000               115.0       407.0   \n",
       "98                               NaN                 NaN         NaN   \n",
       "99                               NaN                 NaN         NaN   \n",
       "\n",
       "    SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0            1.710569               56.0         4.555285  \n",
       "1            2.000000               22.0         5.389853  \n",
       "2            2.255814               17.0         6.086512  \n",
       "3            2.177191               13.0         5.918944  \n",
       "4            1.956459               12.0         5.509434  \n",
       "..                ...                ...              ...  \n",
       "95           1.894690                5.0         5.230973  \n",
       "96           1.696087               27.0         4.623294  \n",
       "97           1.889435                4.0         5.353808  \n",
       "98                NaN                NaN              NaN  \n",
       "99                NaN                NaN              NaN  \n",
       "\n",
       "[100 rows x 15 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b530bd1-d57c-463c-8cd9-edb058576fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
